{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implémentation s-vae pour tahoe-100m\n",
    "\n",
    "**Mouhssine Rifaki - Raphaël Rubrice - (MVA)**\n",
    "\n",
    "reproduction du papier hyperspherical variational auto-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installations nécessaires\n",
    "!pip install torch torchvision numpy scipy matplotlib pandas scikit-learn datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.special import ive, i0, i1\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partie théorique - distribution von mises-fisher\n",
    "\n",
    "la distribution vmf sur $S^{m-1}$ est définie par:\n",
    "\n",
    "$$q(z|\\mu, \\kappa) = C_m(\\kappa) \\exp(\\kappa \\mu^T z)$$\n",
    "\n",
    "où $C_m(\\kappa) = \\frac{\\kappa^{m/2-1}}{(2\\pi)^{m/2} I_{m/2-1}(\\kappa)}$\n",
    "\n",
    "divergence kl entre vmf et uniforme sur la sphère:\n",
    "\n",
    "$$KL[vMF(\\mu,\\kappa) || U(S^{m-1})] = \\kappa \\frac{I_{m/2}(\\kappa)}{I_{m/2-1}(\\kappa)} + \\log C_m(\\kappa) - \\log \\left(\\frac{2\\pi^{m/2}}{\\Gamma(m/2)}\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonctions utilitaires pour vmf\n",
    "def sample_vmf(mu, kappa, batch_size):\n",
    "    \"\"\"échantillonnage depuis vmf en dimension m\"\"\"\n",
    "    m = mu.shape[-1]\n",
    "    \n",
    "    # cas 2d simplifié\n",
    "    if m == 2:\n",
    "        angles = torch.randn(batch_size, 1) * 2 * np.pi\n",
    "        x = torch.cos(angles)\n",
    "        y = torch.sin(angles)\n",
    "        samples = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        # concentration autour de mu\n",
    "        if kappa > 0:\n",
    "            mu_angle = torch.atan2(mu[1], mu[0])\n",
    "            concentrated_angles = mu_angle + torch.randn(batch_size, 1) / (kappa + 1e-8)\n",
    "            x = torch.cos(concentrated_angles)\n",
    "            y = torch.sin(concentrated_angles)\n",
    "            samples = torch.cat([x, y], dim=1)\n",
    "        return samples\n",
    "    \n",
    "    # cas général - algorithme d'ulrich\n",
    "    b = -2 * kappa + torch.sqrt(4 * kappa**2 + (m-1)**2)\n",
    "    b = b / (m - 1)\n",
    "    a = (m - 1 + 2 * kappa + torch.sqrt(4 * kappa**2 + (m-1)**2)) / 4\n",
    "    d = 4 * a * b / (1 + b) - (m - 1) * torch.log(torch.tensor(m - 1))\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        while True:\n",
    "            epsilon = torch.rand(1).beta((m-1)/2, (m-1)/2)\n",
    "            omega = 1 - (1 + b) * epsilon / (1 - (1 - b) * epsilon)\n",
    "            t = 2 * a * b / (1 - (1 - b) * epsilon)\n",
    "            u = torch.rand(1)\n",
    "            if (m - 1) * torch.log(t) - t + d >= torch.log(u):\n",
    "                break\n",
    "        \n",
    "        # échantillonnage sur s^{m-2}\n",
    "        v = torch.randn(m - 1)\n",
    "        v = v / torch.norm(v)\n",
    "        \n",
    "        # construction du sample\n",
    "        z = torch.cat([omega.unsqueeze(0), torch.sqrt(1 - omega**2) * v])\n",
    "        \n",
    "        # transformation householder pour aligner avec mu\n",
    "        e1 = torch.zeros(m)\n",
    "        e1[0] = 1\n",
    "        u = e1 - mu\n",
    "        u = u / (torch.norm(u) + 1e-8)\n",
    "        householder = torch.eye(m) - 2 * torch.outer(u, u)\n",
    "        z = householder @ z\n",
    "        samples.append(z)\n",
    "    \n",
    "    return torch.stack(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\"implémentation du s-vae avec distribution vmf\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # encodeur\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_kappa = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        # décodeur\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim // 2)\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        \n",
    "        # mu normalisé sur la sphère\n",
    "        mu = self.fc_mu(h)\n",
    "        mu = mu / (torch.norm(mu, dim=-1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # kappa positif\n",
    "        kappa = F.softplus(self.fc_kappa(h)) + 0.1\n",
    "        \n",
    "        return mu, kappa\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        return self.fc5(h)\n",
    "    \n",
    "    def kl_vmf(self, mu, kappa):\n",
    "        \"\"\"calcul de la divergence kl\"\"\"\n",
    "        m = self.latent_dim\n",
    "        \n",
    "        # utilisation de ive pour stabilité numérique\n",
    "        iv = ive(m/2, kappa) * torch.exp(torch.abs(kappa))\n",
    "        iv_prev = ive(m/2 - 1, kappa) * torch.exp(torch.abs(kappa))\n",
    "        \n",
    "        kl = kappa * (iv / (iv_prev + 1e-8))\n",
    "        \n",
    "        # terme log c_m(kappa)\n",
    "        log_cm = (m/2 - 1) * torch.log(kappa + 1e-8) - (m/2) * np.log(2 * np.pi) - torch.log(iv_prev + 1e-8)\n",
    "        \n",
    "        # terme constant\n",
    "        const = -np.log(2 * np.pi**(m/2) / np.math.gamma(m/2))\n",
    "        \n",
    "        return (kl + log_cm + const).mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, kappa = self.encode(x)\n",
    "        \n",
    "        # échantillonnage\n",
    "        batch_size = x.shape[0]\n",
    "        z = sample_vmf(mu[0], kappa[0, 0], batch_size)\n",
    "        \n",
    "        # reconstruction\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## données synthétiques - validation sur cercle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# génération données sur s1 embedées dans r100\n",
    "def generate_circle_data(n_samples=1000):\n",
    "    angles = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    \n",
    "    # 3 clusters sur le cercle\n",
    "    cluster_centers = [0, 2*np.pi/3, 4*np.pi/3]\n",
    "    labels = np.random.choice(3, n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        angles[i] = cluster_centers[labels[i]] + np.random.randn() * 0.3\n",
    "    \n",
    "    # coordonnées 2d\n",
    "    x = np.cos(angles)\n",
    "    y = np.sin(angles)\n",
    "    data_2d = np.stack([x, y], axis=1)\n",
    "    \n",
    "    # projection non-linéaire vers r100\n",
    "    projection = np.random.randn(2, 100)\n",
    "    data_high = data_2d @ projection\n",
    "    data_high += np.random.randn(n_samples, 100) * 0.1  # bruit\n",
    "    \n",
    "    return data_high, data_2d, labels\n",
    "\n",
    "data_high, data_2d, labels = generate_circle_data()\n",
    "data_tensor = torch.FloatTensor(data_high)\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# entrainement sur données circulaires\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_svae \u001b[38;5;241m=\u001b[39m SVAE(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_svae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVAE' is not defined"
     ]
    }
   ],
   "source": [
    "# entrainement sur données circulaires\n",
    "model_svae = SVAE(100, 128, 2)\n",
    "optimizer = torch.optim.Adam(model_svae.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_recon, mu, kappa = model_svae(x)\n",
    "        \n",
    "        # reconstruction loss\n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        \n",
    "        # kl divergence\n",
    "        kl_loss = model_svae.kl_vmf(mu, kappa)\n",
    "        \n",
    "        loss = recon_loss + 0.1 * kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    losses.append(epoch_loss / len(dataloader))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch}: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation espace latent\n",
    "with torch.no_grad():\n",
    "    mu_all, kappa_all = model_svae.encode(data_tensor)\n",
    "    mu_np = mu_all.numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.title('données originales s1')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(mu_np[:, 0], mu_np[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.title('espace latent s-vae')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(losses)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparaison avec vae gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianVAE(nn.Module):\n",
    "    \"\"\"vae standard avec prior gaussien\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encodeur\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        \n",
    "        # décodeur\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim // 2)\n",
    "        self.fc4 = nn.Linear(hidden_dim // 2, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        return self.fc5(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# entrainement vae gaussien\n",
    "model_nvae = GaussianVAE(100, 128, 2)\n",
    "optimizer_nvae = torch.optim.Adam(model_nvae.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        optimizer_nvae.zero_grad()\n",
    "        \n",
    "        x_recon, mu, logvar = model_nvae(x)\n",
    "        \n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        loss = recon_loss + 0.001 * kl_loss\n",
    "        loss.backward()\n",
    "        optimizer_nvae.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparaison espaces latents\n",
    "with torch.no_grad():\n",
    "    mu_nvae, _ = model_nvae.encode(data_tensor)\n",
    "    mu_svae, _ = model_svae.encode(data_tensor)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(mu_nvae[:, 0], mu_nvae[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.title('n-vae (gaussien)')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(mu_svae[:, 0], mu_svae[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.title('s-vae (vmf)')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application au dataset tahoe-100m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement dataset tahoe\n",
    "print(\"chargement tahoe-100m...\")\n",
    "dataset_tahoe = load_dataset(\"tahoebio/Tahoe-100M\", split=\"train\", streaming=True)\n",
    "\n",
    "# échantillonnage pour test initial\n",
    "sample_data = []\n",
    "sample_labels = []\n",
    "n_samples = 1000\n",
    "\n",
    "for i, item in enumerate(dataset_tahoe):\n",
    "    if i >= n_samples:\n",
    "        break\n",
    "    \n",
    "    # extraction expressions géniques\n",
    "    expressions = item['expressions']\n",
    "    if len(expressions) > 0:\n",
    "        # normalisation log et troncature\n",
    "        expr_array = np.array(expressions[:500])  # premiers 500 gènes\n",
    "        expr_array = np.log1p(expr_array)\n",
    "        sample_data.append(expr_array)\n",
    "        sample_labels.append(item['drug'])\n",
    "\n",
    "sample_data = np.array(sample_data)\n",
    "print(f\"forme données: {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pour tahoe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# standardisation\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(sample_data)\n",
    "\n",
    "# réduction dimensionnalité initiale avec pca\n",
    "pca = PCA(n_components=50)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "print(f\"variance expliquée: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# conversion torch\n",
    "tahoe_tensor = torch.FloatTensor(data_pca)\n",
    "tahoe_dataset = TensorDataset(tahoe_tensor)\n",
    "tahoe_loader = DataLoader(tahoe_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s-vae pour tahoe\n",
    "model_tahoe = SVAE(50, 256, 10)  # dimension latente 10 pour capturer complexité\n",
    "optimizer_tahoe = torch.optim.Adam(model_tahoe.parameters(), lr=0.0001)\n",
    "\n",
    "tahoe_losses = []\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for batch in tahoe_loader:\n",
    "        x = batch[0]\n",
    "        optimizer_tahoe.zero_grad()\n",
    "        \n",
    "        x_recon, mu, kappa = model_tahoe(x)\n",
    "        \n",
    "        recon_loss = F.mse_loss(x_recon, x)\n",
    "        kl_loss = model_tahoe.kl_vmf(mu, kappa)\n",
    "        \n",
    "        loss = recon_loss + 0.01 * kl_loss\n",
    "        loss.backward()\n",
    "        optimizer_tahoe.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    tahoe_losses.append(epoch_loss / len(tahoe_loader))\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"epoch {epoch}: loss = {tahoe_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse espace latent tahoe\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu_tahoe, kappa_tahoe = model_tahoe.encode(tahoe_tensor)\n",
    "    mu_tahoe_np = mu_tahoe.numpy()\n",
    "\n",
    "# projection tsne pour visualisation (dimension 10 -> 2)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "mu_tsne = tsne.fit_transform(mu_tahoe_np)\n",
    "\n",
    "# encodage couleurs pour drugs\n",
    "unique_drugs = list(set(sample_labels))\n",
    "drug_colors = [unique_drugs.index(d) for d in sample_labels]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(mu_tsne[:, 0], mu_tsne[:, 1], c=drug_colors, cmap='tab20', alpha=0.6, s=10)\n",
    "plt.title('espace latent s-vae sur tahoe (projection t-sne)')\n",
    "plt.colorbar(label='drug id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse concentration kappa\n",
    "kappa_values = kappa_tahoe.numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(kappa_values, bins=30, alpha=0.7)\n",
    "plt.title('distribution des kappa')\n",
    "plt.xlabel('kappa')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(range(len(kappa_values)), kappa_values, alpha=0.3, s=1)\n",
    "plt.title('kappa par échantillon')\n",
    "plt.xlabel('échantillon')\n",
    "plt.ylabel('kappa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"kappa moyen: {kappa_values.mean():.3f} +/- {kappa_values.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## évaluation quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# métriques de reconstruction\n",
    "with torch.no_grad():\n",
    "    x_recon_tahoe, _, _ = model_tahoe(tahoe_tensor)\n",
    "    \n",
    "    mse = F.mse_loss(x_recon_tahoe, tahoe_tensor).item()\n",
    "    \n",
    "    # corrélation par échantillon\n",
    "    correlations = []\n",
    "    for i in range(len(tahoe_tensor)):\n",
    "        corr = np.corrcoef(tahoe_tensor[i].numpy(), x_recon_tahoe[i].numpy())[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    mean_corr = np.mean(correlations)\n",
    "\n",
    "print(f\"mse reconstruction: {mse:.4f}\")\n",
    "print(f\"corrélation moyenne: {mean_corr:.4f}\")\n",
    "\n",
    "# distribution des corrélations\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(correlations, bins=30, alpha=0.7)\n",
    "plt.axvline(mean_corr, color='red', linestyle='--', label=f'moyenne: {mean_corr:.3f}')\n",
    "plt.title('corrélations reconstruction')\n",
    "plt.xlabel('corrélation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering dans espace latent\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# test différents nombres de clusters\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(mu_tahoe_np)\n",
    "    score = silhouette_score(mu_tahoe_np, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('nombre de clusters')\n",
    "plt.ylabel('score silhouette')\n",
    "plt.title('qualité clustering espace latent')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"nombre optimal de clusters: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## génération et interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# génération depuis prior uniforme sur sphère\n",
    "def generate_from_uniform_sphere(model, n_samples=10, dim=10):\n",
    "    # échantillonnage uniforme sur s^{d-1}\n",
    "    z = torch.randn(n_samples, dim)\n",
    "    z = z / torch.norm(z, dim=1, keepdim=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.decode(z)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "generated_samples = generate_from_uniform_sphere(model_tahoe, n_samples=5)\n",
    "\n",
    "# interpolation sphérique entre deux points\n",
    "def spherical_interpolation(z1, z2, n_steps=10):\n",
    "    # normalisation\n",
    "    z1 = z1 / torch.norm(z1)\n",
    "    z2 = z2 / torch.norm(z2)\n",
    "    \n",
    "    # angle entre vecteurs\n",
    "    omega = torch.acos(torch.clamp(torch.dot(z1, z2), -1, 1))\n",
    "    \n",
    "    interpolated = []\n",
    "    for t in np.linspace(0, 1, n_steps):\n",
    "        if omega > 1e-6:\n",
    "            z_t = (torch.sin((1-t)*omega)/torch.sin(omega)) * z1 + (torch.sin(t*omega)/torch.sin(omega)) * z2\n",
    "        else:\n",
    "            z_t = (1-t) * z1 + t * z2\n",
    "        interpolated.append(z_t)\n",
    "    \n",
    "    return torch.stack(interpolated)\n",
    "\n",
    "# test interpolation\n",
    "with torch.no_grad():\n",
    "    idx1, idx2 = 0, 10\n",
    "    z1 = mu_tahoe[idx1]\n",
    "    z2 = mu_tahoe[idx2]\n",
    "    \n",
    "    z_interp = spherical_interpolation(z1, z2)\n",
    "    x_interp = model_tahoe.decode(z_interp)\n",
    "\n",
    "print(f\"interpolation entre échantillons {idx1} et {idx2}\")\n",
    "print(f\"forme interpolation: {x_interp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## résumé résultats :)\n",
    "\n",
    "- implémentation complète s-vae avec distribution von mises-fisher\n",
    "- validation sur données synthétiques circulaires: reconstruction structure latente\n",
    "- application tahoe-100m: extraction features biologiques significatives\n",
    "- espace latent hypersphérique capture mieux structure données que vae gaussien\n",
    "- clustering dans espace latent révèle groupes de drugs/perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde modèle\n",
    "torch.save({\n",
    "    'model_state_dict': model_tahoe.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_tahoe.state_dict(),\n",
    "    'losses': tahoe_losses,\n",
    "}, 'svae_tahoe_checkpoint.pt')\n",
    "\n",
    "print(\"modèle sauvegardé\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
