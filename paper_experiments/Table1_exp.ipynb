{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reproducing Unsupervised results (Table 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/blackswan-advitamaeternam/HVAE/blob/raph/paper_experiments/Table1_exp.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Colab setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# to avoid having the data on your drive\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/blackswan-advitamaeternam/HVAE.git\n",
    "%cd HVAE\n",
    "!git checkout raph\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow automatic reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import imp\n",
    "except ImportError:\n",
    "    import types\n",
    "    sys.modules['imp'] = types.ModuleType('imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# To ensure the custom package is found\n",
    "path_to_repo = \"/content/HVAE\"\n",
    "if path_to_repo not in sys.path:\n",
    "    sys.path.append(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from svae.vae import SVAE, GaussianVAE\n",
    "from svae.training import training\n",
    "from paper_experiments.load_MNIST import load_mnist, ShuffledLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make splits\n",
    "NUM_WORKERS = int(0.8*os.cpu_count())\n",
    "FRAC = 1.0  # Use full dataset for Table 1\n",
    "TRAIN_FRAC = int(50000 * FRAC)\n",
    "VAL_FRAC = int(10000 * FRAC)\n",
    "TEST_FRAC = None\n",
    "print(f\"Using {TRAIN_FRAC} train samples, {VAL_FRAC} val samples\")\n",
    "\n",
    "train_loader, val_loader, test_loader = load_mnist(\n",
    "    train_size=TRAIN_FRAC,\n",
    "    val_size=VAL_FRAC,\n",
    "    test_size=TEST_FRAC,\n",
    "    batch_size=64,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    binarize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifesting to memory of the device\n",
    "train_batches = [[el.to(DEVICE) for el in batch] for batch in train_loader]\n",
    "val_batches   = [[el.to(DEVICE) for el in batch] for batch in val_loader]\n",
    "test_batches  = [[el.to(DEVICE) for el in batch] for batch in test_loader]\n",
    "\n",
    "# Wrap for shuffling behavior\n",
    "train_loader = ShuffledLoader(train_batches, shuffle_batches=True, shuffle_within=True)\n",
    "val_loader = ShuffledLoader(val_batches, shuffle_batches=False, shuffle_within=False)\n",
    "test_loader = ShuffledLoader(test_batches, shuffle_batches=False, shuffle_within=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/HVAE/Table1_results/\"\n",
    "os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters matching paper (Section F.1)\n",
    "EPOCHS = 500\n",
    "INPUT_DIM = 784\n",
    "HIDDEN_DIM = 128  # Paper: [256, 128] encoder, [128, 256] decoder\n",
    "\n",
    "PATIENCE = 50\n",
    "WARMUP = 100  # Paper: linear warm-up for 100 epochs\n",
    "ONE_LAYER = False\n",
    "LR = 1e-3\n",
    "BETA_KL = 1.0\n",
    "\n",
    "N_RUNS = 10  # Paper uses 10 runs\n",
    "N_LL_SAMPLES = 500  # Paper uses 500 importance samples for LL estimation\n",
    "\n",
    "LATENT_DIMS = [2, 5, 10, 20, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, test_tensor, N_ll_samples=500):\n",
    "    \"\"\"\n",
    "    Compute Table 1 metrics for a trained model.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: LL, ELBO (L[q]), RE, KL\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute LL using IWAE estimator (already implemented in your code!)\n",
    "        LL = model.total_marginal_ll(test_tensor, N=N_ll_samples, reduced='mean').item()\n",
    "        \n",
    "        # Compute RE and KL using full_step\n",
    "        _, parts = model.full_step(test_tensor, beta_kl=1.0)\n",
    "        RE = -parts['recon'].item()  # Paper reports negative RE\n",
    "        KL = parts['kl'].item()\n",
    "        \n",
    "        # ELBO = -RE - KL (paper reports L[q] which is negative ELBO)\n",
    "        ELBO = RE - KL  # This gives the paper's L[q]\n",
    "    \n",
    "    return {\n",
    "        'LL': LL,\n",
    "        'ELBO': ELBO,\n",
    "        'RE': RE,\n",
    "        'KL': KL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(mode, latent_dim, train_loader, val_loader, test_tensor, n_ll_samples):\n",
    "    \"\"\"\n",
    "    Train a single model and evaluate Table 1 metrics.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'svae' or 'normal'\n",
    "        latent_dim: dimension of latent space\n",
    "    \n",
    "    Returns:\n",
    "        dict with LL, ELBO, RE, KL\n",
    "    \"\"\"\n",
    "    addon = \"[SVAE]\" if mode == \"svae\" else \"[NVAE]\"\n",
    "    print(f\"\\n{addon} Training with latent_dim={latent_dim}..\")\n",
    "    \n",
    "    # Instantiate model\n",
    "    if mode == \"svae\":\n",
    "        model = SVAE(\n",
    "            input_dim=INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            latent_dim=latent_dim,\n",
    "            one_layer=ONE_LAYER,\n",
    "            mode='mnist'  # Uses BCE loss\n",
    "        )\n",
    "    else:\n",
    "        model = GaussianVAE(\n",
    "            input_dim=INPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            latent_dim=latent_dim,\n",
    "            one_layer=ONE_LAYER,\n",
    "            mode='mnist'\n",
    "        )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Train\n",
    "    model, losses, all_parts = training(\n",
    "        dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        beta_kl=BETA_KL,\n",
    "        warmup=WARMUP,\n",
    "        patience=PATIENCE,\n",
    "        show_loss_every=50\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"{addon} Computing metrics with {n_ll_samples} importance samples..\")\n",
    "    metrics = compute_metrics(model, test_tensor, N_ll_samples=n_ll_samples)\n",
    "    \n",
    "    print(f\"{addon} LL={metrics['LL']:.2f}, ELBO={metrics['ELBO']:.2f}, RE={metrics['RE']:.2f}, KL={metrics['KL']:.2f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_table1_experiment(latent_dims, n_runs, n_ll_samples, train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Run the complete Table 1 experiment.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    # Prepare test tensor (full test set)\n",
    "    test_data = [batch[0] for batch in test_loader]\n",
    "    test_tensor = torch.cat(test_data, dim=0).to(DEVICE)\n",
    "    print(f\"Test tensor shape: {test_tensor.shape}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for latent_dim in tqdm(latent_dims, desc=\"Latent dimensions\"):\n",
    "        for mode in [\"normal\", \"svae\"]:\n",
    "            model_name = \"S-VAE\" if mode == \"svae\" else \"N-VAE\"\n",
    "            \n",
    "            # Collect metrics over runs\n",
    "            run_metrics = {'LL': [], 'ELBO': [], 'RE': [], 'KL': []}\n",
    "            \n",
    "            for run in tqdm(range(n_runs), desc=f\"{model_name} d={latent_dim}\", leave=False):\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"RUN {run+1}/{n_runs} | {model_name} | d={latent_dim}\")\n",
    "                print('='*50)\n",
    "                \n",
    "                metrics = train_and_evaluate(\n",
    "                    mode=mode,\n",
    "                    latent_dim=latent_dim,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    test_tensor=test_tensor,\n",
    "                    n_ll_samples=n_ll_samples\n",
    "                )\n",
    "                \n",
    "                for key in run_metrics:\n",
    "                    run_metrics[key].append(metrics[key])\n",
    "            \n",
    "            # Compute mean and std\n",
    "            results.append({\n",
    "                'Method': model_name,\n",
    "                'd': latent_dim,\n",
    "                'LL': f\"{np.mean(run_metrics['LL']):.2f}±{np.std(run_metrics['LL']):.2f}\",\n",
    "                'L[q]': f\"{np.mean(run_metrics['ELBO']):.2f}±{np.std(run_metrics['ELBO']):.2f}\",\n",
    "                'RE': f\"{np.mean(run_metrics['RE']):.2f}±{np.std(run_metrics['RE']):.2f}\",\n",
    "                'KL': f\"{np.mean(run_metrics['KL']):.2f}±{np.std(run_metrics['KL']):.2f}\",\n",
    "                # Raw values for analysis\n",
    "                'LL_mean': np.mean(run_metrics['LL']),\n",
    "                'LL_std': np.std(run_metrics['LL']),\n",
    "                'ELBO_mean': np.mean(run_metrics['ELBO']),\n",
    "                'ELBO_std': np.std(run_metrics['ELBO']),\n",
    "                'RE_mean': np.mean(run_metrics['RE']),\n",
    "                'RE_std': np.std(run_metrics['RE']),\n",
    "                'KL_mean': np.mean(run_metrics['KL']),\n",
    "                'KL_std': np.std(run_metrics['KL'])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = run_table1_experiment(\n",
    "    latent_dims=LATENT_DIMS,\n",
    "    n_runs=N_RUNS,\n",
    "    n_ll_samples=N_LL_SAMPLES,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv(base_path + \"Table1_results.csv\", index=False)\n",
    "print(f\"Results saved to {base_path}Table1_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Display Results (Table 1 Format)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format as paper's Table 1\n",
    "display_df = results_df[['Method', 'd', 'LL', 'L[q]', 'RE', 'KL']].copy()\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for side-by-side comparison like in the paper\n",
    "def format_table1_paper_style(df):\n",
    "    \"\"\"Format results exactly like Table 1 in the paper\"\"\"\n",
    "    nvae = df[df['Method'] == 'N-VAE'].set_index('d')\n",
    "    svae = df[df['Method'] == 'S-VAE'].set_index('d')\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"TABLE 1: Summary of results (mean and standard-deviation over runs) of unsupervised model on MNIST\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Method':<10} | {'LL':^20} | {'L[q]':^20} | {'RE':^20} | {'KL':^15}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for d in LATENT_DIMS:\n",
    "        # N-VAE row\n",
    "        print(f\"{'N-VAE':<7} d={d:<2} | {nvae.loc[d, 'LL']:^20} | {nvae.loc[d, 'L[q]']:^20} | {nvae.loc[d, 'RE']:^20} | {nvae.loc[d, 'KL']:^15}\")\n",
    "        # S-VAE row\n",
    "        print(f\"{'S-VAE':<7} d={d:<2} | {svae.loc[d, 'LL']:^20} | {svae.loc[d, 'L[q]']:^20} | {svae.loc[d, 'RE']:^20} | {svae.loc[d, 'KL']:^15}\")\n",
    "        print(\"-\"*100)\n",
    "\n",
    "format_table1_paper_style(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate LaTeX Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(df):\n",
    "    \"\"\"Generate LaTeX code for Table 1\"\"\"\n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Summary of results (mean and standard-deviation over \"\"\" + str(N_RUNS) + r\"\"\" runs) of unsupervised model on MNIST.}\n",
    "\\begin{tabular}{ll|cccc}\n",
    "\\toprule\n",
    "Method & $d$ & LL & $\\mathcal{L}[q]$ & RE & KL \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    for d in LATENT_DIMS:\n",
    "        for method in ['N-VAE', 'S-VAE']:\n",
    "            row = df[(df['Method'] == method) & (df['d'] == d)].iloc[0]\n",
    "            prefix = r\"$\\mathcal{N}$\" if method == 'N-VAE' else r\"$\\mathcal{S}$\"\n",
    "            \n",
    "            # Format with bold for best\n",
    "            latex += f\"{prefix}-VAE & {d} & {row['LL']} & {row['L[q]']} & {row['RE']} & {row['KL']} \\\\\\\\\\n\"\n",
    "        \n",
    "        if d != LATENT_DIMS[-1]:\n",
    "            latex += r\"\\midrule\" + \"\\n\"\n",
    "    \n",
    "    latex += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    return latex\n",
    "\n",
    "latex_table = generate_latex_table(results_df)\n",
    "print(latex_table)\n",
    "\n",
    "# Save LaTeX\n",
    "with open(base_path + \"Table1_latex.tex\", 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"\\nLaTeX saved to {base_path}Table1_latex.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quick Comparison with Paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper's Table 1 results for reference\n",
    "paper_results = {\n",
    "    'N-VAE': {\n",
    "        2: {'LL': -135.73, 'ELBO': -137.08, 'RE': -129.84, 'KL': 7.24},\n",
    "        5: {'LL': -110.21, 'ELBO': -112.98, 'RE': -100.16, 'KL': 12.82},\n",
    "        10: {'LL': -93.84, 'ELBO': -98.36, 'RE': -78.93, 'KL': 19.44},\n",
    "        20: {'LL': -88.90, 'ELBO': -94.79, 'RE': -71.29, 'KL': 23.50},\n",
    "        40: {'LL': -88.93, 'ELBO': -94.91, 'RE': -71.14, 'KL': 23.77}\n",
    "    },\n",
    "    'S-VAE': {\n",
    "        2: {'LL': -132.50, 'ELBO': -133.72, 'RE': -126.43, 'KL': 7.28},\n",
    "        5: {'LL': -108.43, 'ELBO': -111.19, 'RE': -97.84, 'KL': 13.35},\n",
    "        10: {'LL': -93.16, 'ELBO': -97.70, 'RE': -77.03, 'KL': 20.67},\n",
    "        20: {'LL': -89.02, 'ELBO': -96.15, 'RE': -67.65, 'KL': 28.50},\n",
    "        40: {'LL': -90.87, 'ELBO': -101.26, 'RE': -67.75, 'KL': 33.50}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Comparison: Our Results vs Paper\")\n",
    "print(\"=\"*80)\n",
    "for d in LATENT_DIMS:\n",
    "    for method in ['N-VAE', 'S-VAE']:\n",
    "        our = results_df[(results_df['Method'] == method) & (results_df['d'] == d)].iloc[0]\n",
    "        paper = paper_results[method][d]\n",
    "        \n",
    "        print(f\"{method} d={d}:\")\n",
    "        print(f\"  LL:   Ours={our['LL_mean']:.2f} | Paper={paper['LL']:.2f} | Δ={our['LL_mean']-paper['LL']:.2f}\")\n",
    "        print(f\"  RE:   Ours={our['RE_mean']:.2f} | Paper={paper['RE']:.2f} | Δ={our['RE_mean']-paper['RE']:.2f}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
